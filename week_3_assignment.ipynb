{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7e1bf9",
   "metadata": {},
   "source": [
    "# Search Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c91ff",
   "metadata": {},
   "source": [
    "# Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce47d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in c:\\users\\chungke2\\appdata\\local\\anaconda3\\envs\\evaluation\\lib\\site-packages (0.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.18 environment at: C:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m33 packages\u001b[0m \u001b[2min 323ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m33 packages\u001b[0m \u001b[2min 0.27ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uv\n",
    "!uv pip install -U minsearch qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02f16a",
   "metadata": {},
   "source": [
    "# Evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1585053d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.18 environment at: C:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 199ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced6dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69a456",
   "metadata": {},
   "source": [
    "documents contains the documents from the FAQ database with unique IDs, and ground_truth contains generated question-answer pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10543e71",
   "metadata": {},
   "source": [
    "Also, we will need the code for evaluating retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6425a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.18 environment at: C:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 122ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 421ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c00c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed094c",
   "metadata": {},
   "source": [
    "# Q1. Minsearch text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd16c21",
   "metadata": {},
   "source": [
    "Now let's evaluate our usual minsearch approach, but tweak the parameters. Let's use the following boosting params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e81af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b7654c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x28c8547c670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\", \"id\"],\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620e9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:22<00:00, 207.03it/s]\n",
      "100%|██████████| 4627/4627 [00:22<00:00, 207.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate: 0.848714069591528\n"
     ]
    }
   ],
   "source": [
    "boost_dict = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "def search_function(q):\n",
    "    # q is a dict from ground_truth, e.g. {'question': ..., 'course': ..., ...}\n",
    "    return index.search(\n",
    "        query=q['question'],\n",
    "        filter_dict={'course': q['course']},\n",
    "        boost_dict=boost_dict,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "result = evaluate(ground_truth, search_function)\n",
    "print(\"Hitrate:\", result['hit_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ce44a",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec47fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8ec1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50ed58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create embeddings for the \"question\" field:\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f5789",
   "metadata": {},
   "source": [
    "# Q2. Vector search for question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99789e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x28cffb521a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index these embeddings with minsearch:\n",
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e12dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:12<00:00, 382.24it/s]\n",
      "100%|██████████| 4627/4627 [00:12<00:00, 382.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.36761837866765423\n"
     ]
    }
   ],
   "source": [
    "def vector_search_function(q):\n",
    "    # Transform the query question using the pipeline\n",
    "    query_vec = pipeline.transform([q['question']])[0]\n",
    "    # Perform vector search with filter\n",
    "    return vindex.search(query_vec, filter_dict={'course': q['course']})\n",
    "\n",
    "result = evaluate(ground_truth, vector_search_function)\n",
    "print(\"MRR:\", result['mrr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289d90a",
   "metadata": {},
   "source": [
    "# Q3 Vector search for question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8845431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4627/4627 [00:17<00:00, 263.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate: 0.8841582018586557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create combined question+answer texts\n",
    "texts = []\n",
    "for doc in documents:\n",
    "    t = doc['question'] + ' ' + doc['text']\n",
    "    texts.append(t)\n",
    "\n",
    "# Create embeddings\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Index embeddings with minsearch\n",
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)\n",
    "\n",
    "# Define search function - only use question for query\n",
    "def vector_search_function(q):\n",
    "    query_vec = pipeline.transform([q['question']]).flatten()\n",
    "    return vindex.search(query_vec, filter_dict={'course': q['course']})\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate(ground_truth, vector_search_function)\n",
    "print(\"Hitrate:\", result['hit_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94e05d",
   "metadata": {},
   "source": [
    "# Q4. Qdrant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70f15ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.18 environment at: C:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m48 packages\u001b[0m \u001b[2min 534ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m transformers \u001b[2m(10.3MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m transformers\n",
      "\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 7.63s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m16 packages\u001b[0m \u001b[2min 14.94s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.33.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Q4. Qdrant\n",
    "!uv pip install qdrant-client sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32f9c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87779fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Qdrant client (in-memory)\n",
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9bc58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v2-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "922cca5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Collection documents already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create collection\u001b[39;00m\n\u001b[0;32m      2\u001b[0m collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVectorParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOSINE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\\lib\\site-packages\\qdrant_client\\qdrant_client.py:2382\u001b[0m, in \u001b[0;36mQdrantClient.create_collection\u001b[1;34m(self, collection_name, vectors_config, sparse_vectors_config, shard_number, sharding_method, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, timeout, strict_mode_config, **kwargs)\u001b[0m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create empty collection with given parameters\u001b[39;00m\n\u001b[0;32m   2333\u001b[0m \n\u001b[0;32m   2334\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2378\u001b[0m \u001b[38;5;124;03m    Operation result\u001b[39;00m\n\u001b[0;32m   2379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2380\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m   2383\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m   2384\u001b[0m     vectors_config\u001b[38;5;241m=\u001b[39mvectors_config,\n\u001b[0;32m   2385\u001b[0m     shard_number\u001b[38;5;241m=\u001b[39mshard_number,\n\u001b[0;32m   2386\u001b[0m     sharding_method\u001b[38;5;241m=\u001b[39msharding_method,\n\u001b[0;32m   2387\u001b[0m     replication_factor\u001b[38;5;241m=\u001b[39mreplication_factor,\n\u001b[0;32m   2388\u001b[0m     write_consistency_factor\u001b[38;5;241m=\u001b[39mwrite_consistency_factor,\n\u001b[0;32m   2389\u001b[0m     on_disk_payload\u001b[38;5;241m=\u001b[39mon_disk_payload,\n\u001b[0;32m   2390\u001b[0m     hnsw_config\u001b[38;5;241m=\u001b[39mhnsw_config,\n\u001b[0;32m   2391\u001b[0m     optimizers_config\u001b[38;5;241m=\u001b[39moptimizers_config,\n\u001b[0;32m   2392\u001b[0m     wal_config\u001b[38;5;241m=\u001b[39mwal_config,\n\u001b[0;32m   2393\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,\n\u001b[0;32m   2394\u001b[0m     init_from\u001b[38;5;241m=\u001b[39minit_from,\n\u001b[0;32m   2395\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   2396\u001b[0m     sparse_vectors_config\u001b[38;5;241m=\u001b[39msparse_vectors_config,\n\u001b[0;32m   2397\u001b[0m     strict_mode_config\u001b[38;5;241m=\u001b[39mstrict_mode_config,\n\u001b[0;32m   2398\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2399\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\\lib\\site-packages\\qdrant_client\\local\\qdrant_local.py:1010\u001b[0m, in \u001b[0;36mQdrantLocal.create_collection\u001b[1;34m(self, collection_name, vectors_config, init_from, sparse_vectors_config, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m     src_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collection(from_collection_name)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collection_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollections:\n\u001b[1;32m-> 1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1011\u001b[0m collection_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_path(collection_name)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collection_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Collection documents already exists"
     ]
    }
   ],
   "source": [
    "# Create collection\n",
    "collection_name = \"documents\"\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9962becb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare documents and embeddings\n",
    "texts = []\n",
    "for doc in documents:\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    texts.append(text)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Index documents\n",
    "points = []\n",
    "for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embedding.tolist(),\n",
    "        payload={\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"course\": doc[\"course\"],\n",
    "            \"question\": doc[\"question\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"section\": doc[\"section\"]\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b196b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4627 [00:00<?, ?it/s]C:\\Users\\CHUNGKE2\\AppData\\Local\\Temp\\ipykernel_28164\\3853615257.py:7: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "  0%|          | 2/4627 [00:00<03:51, 19.96it/s]C:\\Users\\CHUNGKE2\\AppData\\Local\\Temp\\ipykernel_28164\\3853615257.py:7: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "100%|██████████| 4627/4627 [03:47<00:00, 20.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.15411713853468792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define search function\n",
    "def qdrant_search_function(q):\n",
    "    # Create embedding for the query\n",
    "    query_embedding = model.encode([q['question']])\n",
    "    \n",
    "    # Search in Qdrant with proper filter syntax\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding[0].tolist(),\n",
    "        query_filter=Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=MatchValue(value=q['course'])\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Convert results to the expected format\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        formatted_results.append({\n",
    "            \"id\": result.payload[\"id\"],\n",
    "            \"course\": result.payload[\"course\"],\n",
    "            \"question\": result.payload[\"question\"],\n",
    "            \"text\": result.payload[\"text\"],\n",
    "            \"section\": result.payload[\"section\"]\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate(ground_truth, qdrant_search_function)\n",
    "print(\"MRR:\", result['mrr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731875f9",
   "metadata": {},
   "source": [
    "# Q5. Cosine simiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82dd6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Cosine similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31d66582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity: 0.842\n"
     ]
    }
   ],
   "source": [
    "# Load the results data\n",
    "results_url = url_prefix + 'rag_evaluation/data/results-gpt4o-mini.csv'\n",
    "df_results = pd.read_csv(results_url)\n",
    "\n",
    "# Create the pipeline for embeddings\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "\n",
    "# Fit the pipeline on all text data\n",
    "all_text = df_results.answer_llm + ' ' + df_results.answer_orig + ' ' + df_results.question\n",
    "pipeline.fit(all_text)\n",
    "\n",
    "# Define cosine similarity function\n",
    "def cosine(u, v):\n",
    "    u_norm = np.sqrt(u.dot(u))\n",
    "    v_norm = np.sqrt(v.dot(v))\n",
    "    return u.dot(v) / (u_norm * v_norm)\n",
    "\n",
    "# Calculate cosine similarities\n",
    "cosine_similarities = []\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    # Create embeddings for LLM answer and original answer\n",
    "    v_llm = pipeline.transform([row.answer_llm])[0]\n",
    "    v_orig = pipeline.transform([row.answer_orig])[0]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine(v_llm, v_orig)\n",
    "    cosine_similarities.append(similarity)\n",
    "\n",
    "# Calculate average cosine similarity\n",
    "average_cosine = np.mean(cosine_similarities)\n",
    "print(f\"Average cosine similarity: {average_cosine:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885db78e",
   "metadata": {},
   "source": [
    "# Q6. Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b91c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.18 environment at: C:\\Users\\CHUNGKE2\\AppData\\Local\\anaconda3\\envs\\evaluation\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 368ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 425ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrouge\u001b[0m\u001b[2m==1.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7c2f839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "r = df_results.iloc[10]\n",
    "scores = rouge_scorer.get_scores(r.answer_llm, r.answer_orig)[0]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "425f4f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ROUGE scores for document 10:\n",
      "{'rouge-1': {'r': 0.45454545454545453, 'p': 0.45454545454545453, 'f': 0.45454544954545456}, 'rouge-2': {'r': 0.21621621621621623, 'p': 0.21621621621621623, 'f': 0.21621621121621637}, 'rouge-l': {'r': 0.3939393939393939, 'p': 0.3939393939393939, 'f': 0.393939388939394}}\n",
      "Rouge-1 F1 for document 10: 0.45\n",
      "\n",
      "Average ROUGE-1 F1 score: 0.352\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample ROUGE scores for document 10:\")\n",
    "print(scores)\n",
    "print(f\"Rouge-1 F1 for document 10: {scores['rouge-1']['f']:.2f}\")\n",
    "\n",
    "# Calculate ROUGE-1 F1 for all pairs\n",
    "rouge_1_f1_scores = []\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    try:\n",
    "        scores = rouge_scorer.get_scores(row.answer_llm, row.answer_orig)[0]\n",
    "        rouge_1_f1 = scores['rouge-1']['f']\n",
    "        rouge_1_f1_scores.append(rouge_1_f1)\n",
    "    except:\n",
    "        # Handle any potential errors (empty strings, etc.)\n",
    "        rouge_1_f1_scores.append(0.0)\n",
    "\n",
    "# Calculate average ROUGE-1 F1\n",
    "average_rouge_1_f1 = np.mean(rouge_1_f1_scores)\n",
    "print(f\"\\nAverage ROUGE-1 F1 score: {average_rouge_1_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
